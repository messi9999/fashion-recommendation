{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gdown\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import accuracy, top_k_accuracy\n",
    "from annoy import AnnoyIndex\n",
    "import zipfile\n",
    "import time\n",
    "from google.colab import drive\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the meta data\n",
    "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pWnFiNlNGTVloLUk'\n",
    "output = 'list_category_cloth.txt'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pTGNoWkhZeVpzbFk'\n",
    "output = 'list_category_img.txt'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pdS1FMlNreEwtc1E'\n",
    "output = 'list_eval_partition.txt'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the images\n",
    "root_path = './'\n",
    "url = 'https://drive.google.com/uc?id=1j5fCPgh0gnY6v7ChkWlgnnHH6unxuAbb'\n",
    "output = 'img.zip'\n",
    "gdown.download(url, output, quiet=False)\n",
    "with zipfile.ZipFile(\"img.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = []\n",
    "image_path_list = []\n",
    "data_type_list = []\n",
    "# category names\n",
    "with open('list_category_cloth.txt', 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if i &gt; 1:\n",
    "            category_list.append(line.split(' ')[0])\n",
    "\n",
    "# category map\n",
    "with open('list_category_img.txt', 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if i &gt; 1:\n",
    "            image_path_list.append([word.strip() for word in line.split(' ') if len(word) &gt; 0])\n",
    "\n",
    "\n",
    "# train, valid, test\n",
    "with open('list_eval_partition.txt', 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if i &gt; 1:\n",
    "            data_type_list.append([word.strip() for word in line.split(' ') if len(word) &gt; 0])\n",
    "\n",
    "data_df = pd.DataFrame(image_path_list, columns=['image_path', 'category_number'])\n",
    "data_df['category_number'] = data_df['category_number'].astype(int)\n",
    "data_df = data_df.merge(pd.DataFrame(data_type_list, columns=['image_path', 'dataset_type']), on='image_path')\n",
    "data_df['category'] = data_df['category_number'].apply(lambda x: category_list[int(x) - 1])\n",
    "data_df = data_df.drop('category_number', axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_list = ImageList.from_df(df=data_df, path=root_path, cols='image_path').split_by_idxs(\n",
    "    (data_df[data_df['dataset_type']=='train'].index),\n",
    "    (data_df[data_df['dataset_type']=='val'].index)).label_from_df(cols='category')\n",
    "test_image_list = ImageList.from_df(df=data_df[data_df['dataset_type'] == 'test'], path=root_path, cols='image_path')\n",
    "\n",
    "data = train_image_list.transform(get_transforms(), size=224).databunch(bs=128).normalize(imagenet_stats)\n",
    "data.add_test(test_image_list)\n",
    "data.show_batch(rows=3, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see models available: https://docs.fast.ai/vision.models.html\n",
    "# many options for Resnet, the numbers are the number of layers. \n",
    "# More layers are generally more accurate but take longer to train: resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "# get top 1 and top 5 accuracy\n",
    "def train_model(data, pretrained_model, model_metrics):\n",
    "    learner = cnn_learner(data, pretrained_model, metrics=model_metrics)\n",
    "    learner.model = torch.nn.DataParallel(learner.model)\n",
    "    learner.lr_find()\n",
    "    learner.recorder.plot(suggestion=True)\n",
    "    return learner\n",
    "\n",
    "pretrained_model = models.resnet18 # simple model that can be trained on free tier\n",
    "# pretrained_model = models.resnet50 # need pro tier, model I used\n",
    "\n",
    "model_metrics = [accuracy, partial(top_k_accuracy, k=1), partial(top_k_accuracy, k=5)]\n",
    "learner = train_model(data, pretrained_model, model_metrics)\n",
    "learner.fit_one_cycle(10, max_lr=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learner)\n",
    "interp.plot_top_losses(9, largest=False, figsize=(15,11), heatmap_thresh=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model (temporary, will lose model once environment resets)\n",
    "learner.save('resnet-fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): \n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "        self.features = None\n",
    "    def hook_fn(self, module, input, output): \n",
    "        out = output.detach().cpu().numpy()\n",
    "        if isinstance(self.features, type(None)):\n",
    "            self.features = out\n",
    "        else:\n",
    "            self.features = np.row_stack((self.features, out))\n",
    "    def remove(self): \n",
    "        self.hook.remove()\n",
    "   \n",
    "  # load the trained model\n",
    "def load_learner(data, pretrained_model, model_metrics, model_path):\n",
    "    learner = cnn_learner(data, pretrained_model, metrics=model_metrics)\n",
    "    learner.model = torch.nn.DataParallel(learner.model)\n",
    "    learner = learner.load(model_path)\n",
    "    return learner\n",
    "\n",
    "pretrained_model = models.resnet18 # simple model that can be trained on free tier\n",
    "# pretrained_model = models.resnet50 # need pro tier\n",
    "\n",
    "model_metrics = [accuracy, partial(top_k_accuracy, k=1), partial(top_k_accuracy, k=5)]\n",
    "# if gdrive not mounted:\n",
    "drive.mount('/content/gdrive') \n",
    "\n",
    "\n",
    "model_path = \"/content/gdrive/My Drive/resnet18-fashion\"\n",
    "# model_path = \"/content/gdrive/My Drive/resnet50-fashion\"\n",
    "learner = load_learner(data, pretrained_model, model_metrics, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes time to populate the embeddings for each image\n",
    "# Get 2nd last layer of the model that stores the embedding for the image representations\n",
    "# the last linear layer is the output layer.\n",
    "saved_features = SaveFeatures(learner.model.module[1][4])\n",
    "_= learner.get_preds(data.train_ds)\n",
    "_= learner.get_preds(DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for generating recommendations (exlcude test data)\n",
    "# get the embeddings from trained model\n",
    "img_path = [str(x) for x in (list(data.train_ds.items) +list(data.valid_ds.items))]\n",
    "label = [data.classes[x] for x in (list(data.train_ds.y.items) +list(data.valid_ds.y.items))]\n",
    "label_id = [x for x in (list(data.train_ds.y.items) +list(data.valid_ds.y.items))]\n",
    "data_df_ouput = pd.DataFrame({'img_path': img_path, 'label': label, 'label_id': label_id})\n",
    "data_df_ouput['embeddings'] = np.array(saved_features.features).tolist()\n",
    "# Using Spotify's Annoy\n",
    "def get_similar_images_annoy(annoy_tree, img_index, number_of_items=12):\n",
    "    start = time.time()\n",
    "    img_id, img_label  = data_df_ouput.iloc[img_index, [0, 1]]\n",
    "    similar_img_ids = annoy_tree.get_nns_by_item(img_index, number_of_items+1)\n",
    "    end = time.time()\n",
    "    print(f'{(end - start) * 1000} ms')\n",
    "    # ignore first item as it is always target image\n",
    "    return img_id, img_label, data_df_ouput.iloc[similar_img_ids[1:]] \n",
    "\n",
    "\n",
    "# for images similar to centroid \n",
    "def get_similar_images_annoy_centroid(annoy_tree, vector_value, number_of_items=12):\n",
    "    start = time.time()\n",
    "    similar_img_ids = annoy_tree.get_nns_by_vector(vector_value, number_of_items+1)\n",
    "    end = time.time()\n",
    "    print(f'{(end - start) * 1000} ms')\n",
    "    # ignore first item as it is always target image\n",
    "    return data_df_ouput.iloc[similar_img_ids[1:]] \n",
    "\n",
    "\n",
    "def show_similar_images(similar_images_df, fig_size=[10,10], hide_labels=True):\n",
    "    if hide_labels:\n",
    "        category_list = []\n",
    "        for i in range(len(similar_images_df)):\n",
    "            # replace category with blank so it wont show in display\n",
    "            category_list.append(CategoryList(similar_images_df['label_id'].values*0,\n",
    "                                              [''] * len(similar_images_df)).get(i))\n",
    "    else:\n",
    "        category_list = [learner.data.train_ds.y.reconstruct(y) for y in similar_images_df['label_id']]\n",
    "    return learner.data.show_xys([open_image(img_id) for img_id in similar_images_df['img_path']],\n",
    "                                category_list, figsize=fig_size)\n",
    "  # more tree = better approximation\n",
    "ntree = 100\n",
    "#\"angular\", \"euclidean\", \"manhattan\", \"hamming\", or \"dot\"\n",
    "metric_choice = 'angular'\n",
    "\n",
    "annoy_tree = AnnoyIndex(len(data_df_ouput['embeddings'][0]), metric=metric_choice)\n",
    "\n",
    "# # takes a while to build the tree\n",
    "for i, vector in enumerate(data_df_ouput['embeddings']):\n",
    "    annoy_tree.add_item(i, vector)\n",
    "_  = annoy_tree.build(ntree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_embedding(outfit_embedding_list):\n",
    "    number_of_outfits = outfit_embedding_list.shape[0]\n",
    "    length_of_embedding = outfit_embedding_list.shape[1]\n",
    "    centroid = []\n",
    "    for i in range(length_of_embedding):\n",
    "        centroid.append(np.sum(outfit_embedding_list[:, i])/number_of_outfits)\n",
    "    return centroid\n",
    " # shorts\n",
    "outfit_img_ids = [109938, 106385, 113703, 98666, 113467, 120667, 20840, 8450, 142843, 238607, 124505,222671]\n",
    "outfit_embedding_list = []\n",
    "for img_index in outfit_img_ids:\n",
    "    outfit_embedding_list.append(data_df_ouput.iloc[img_index, 3])\n",
    "\n",
    "outfit_embedding_list = np.array(outfit_embedding_list)\n",
    "outfit_centroid_embedding = centroid_embedding(outfit_embedding_list)\n",
    "outfits_selected = data_df_ouput.iloc[outfit_img_ids] \n",
    "\n",
    "similar_images_df = get_similar_images_annoy_centroid(annoy_tree, outfit_centroid_embedding, 30)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
